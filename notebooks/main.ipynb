{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "from utils.preprocessing import *\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.backend import *\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior() # enables np methods on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_dir = '../data/C-NMC_Leukemia/testing_data/C-NMC_test_final_phase_data' ### WE DO NOT USE THIS\n",
    "training_dir_base = f'../data/C-NMC_Leukemia/training_data/' \n",
    "fold_0_all_path = 'fold_0/all'\n",
    "fold_0_hem_path = 'fold_0/hem'\n",
    "fold_1_all_path = 'fold_1/all'\n",
    "fold_1_hem_path = 'fold_1/hem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The code for this lives in notebooks.utils.preprocessing\n",
    "### Get grayscale images as tensors, and labels for both classes\n",
    "img_train_cancer = get_gray_images(training_dir_base+fold_0_all_path)\n",
    "img_train_healthy = get_gray_images(training_dir_base+fold_0_hem_path)\n",
    "train_health_labels = [0 for _ in range(len(img_train_healthy))]\n",
    "train_cancer_labels = [1 for _ in range(len(img_train_cancer))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestAccuracy(img_cancer, img_healthy, labels_healthy, labels_cancer, nest=100, mssplit=2, mdepth=None, rstate=100):\n",
    "    ### For random forests, X is our combined image set and y is our combined label set.\n",
    "    X =  np.array(img_cancer + img_healthy)\n",
    "    y = np.array(labels_cancer + labels_healthy)\n",
    "\n",
    "    ### train-test-split our data, and initialize the forest.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=100)\n",
    "\n",
    "    forest = RandomForestClassifier(n_estimators=nest, random_state=rstate, min_samples_split=mssplit, max_depth=100)\n",
    "\n",
    "    ### The random forests algorithm expects 2 dimensional data at the most.\n",
    "    ### We can accomplish this on our 4-dimensional data by multiplying the last 3 dimensions together to flatten the data\n",
    "    X_shape = X_train.shape\n",
    "    X_tshape = X_test.shape\n",
    "    X_train = X_train.reshape(X_shape[0], X_shape[1] * X_shape[2] * X_shape[3])\n",
    "    X_test = X_test.reshape(X_tshape[0], X_tshape[1] * X_tshape[2] * X_tshape[3])\n",
    "\n",
    "    ### Fit the random forest, try it on the test data, and assess the model's accuracy\n",
    "    forest.fit(X_train,y_train)\n",
    "    predictions = forest.predict(X_test)\n",
    "    return metrics.accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_original = randomForestAccuracy(img_train_cancer, img_train_healthy, train_health_labels, train_cancer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forests, color\n",
    "### We will repeat the steps above, with color images instead.\n",
    "img_train_cancer = get_gray_images(training_dir_base+fold_0_all_path, 128)\n",
    "img_train_healthy = get_gray_images(training_dir_base+fold_0_hem_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_128 = randomForestAccuracy(img_train_cancer, img_train_healthy, train_health_labels, train_cancer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8593012275731823\n",
      "0.8630783758262511\n"
     ]
    }
   ],
   "source": [
    "### I find that there is a negligible (0.4%) difference between full size and 128x128 downscaled images.\n",
    "### Because downscaled images process faster, I will use those\n",
    "print(accuracy_original)\n",
    "print(accuracy_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assemble combinations of hyperparameters for tuning.\n",
    "from itertools import product\n",
    "_n_estimators = [10, 50, 100]\n",
    "_min_samples_split = [2, 5, 10]\n",
    "_max_depth = [10, 100, None]\n",
    "_random_state = [50, 100, 256]\n",
    "list_of_lists = [_n_estimators, _min_samples_split, _max_depth, _random_state]\n",
    "all_combinations = list(product(*list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the highest accuracy combination of hyperparameters.\n",
    "max_accuracy = -1\n",
    "max_accuracy_params = []\n",
    "for combination in all_combinations:\n",
    "    accuracy = randomForestAccuracy(img_train_cancer, img_train_healthy, train_health_labels, train_cancer_labels, \n",
    "                                    nest=combination[0], mssplit=combination[1], mdepth=combination[2], rstate=combination[3])\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_accuracy_params = combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8649669499527857\n",
      "(100, 10, 10, 100)\n"
     ]
    }
   ],
   "source": [
    "print(max_accuracy)\n",
    "print(max_accuracy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron: Grayscale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayCancer = get_gray_images(training_dir_base+fold_0_all_path)\n",
    "grayHealthy = get_gray_images(training_dir_base+fold_0_hem_path)\n",
    "\n",
    "trainCancerLabels = [1 for _ in range(len(grayCancer))]\n",
    "trainHealthLabels = [0 for _ in range(len(grayHealthy))]\n",
    "\n",
    "images, labels = grayCancer + grayHealthy, trainCancerLabels + trainHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "grayImages = np.array(images).reshape(len(images), 450, 450)\n",
    "grayLabels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle labels and images in unison so they maintain their image-label correspondance\n",
    "randomize = np.arange(len(grayImages))\n",
    "np.random.shuffle(randomize)\n",
    "grayImages = grayImages[randomize]\n",
    "grayLabels = grayLabels[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 120.5793 - accuracy: 0.6988\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 103.6099 - accuracy: 0.7328\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 82.4331 - accuracy: 0.7348\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 71.7083 - accuracy: 0.7727\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 105.3421 - accuracy: 0.7493\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 62.4301 - accuracy: 0.7952\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 30.1126 - accuracy: 0.8272\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 46.0403 - accuracy: 0.8092\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 34.5220 - accuracy: 0.8352\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 41.7521 - accuracy: 0.8227\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 62.4358 - accuracy: 0.8047\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 59.4788 - accuracy: 0.8087\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 38.4776 - accuracy: 0.8407\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 37.7333 - accuracy: 0.8477\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 44.5791 - accuracy: 0.8427\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 24.5374 - accuracy: 0.8706\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 28.2008 - accuracy: 0.8566\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 21.2958 - accuracy: 0.8731\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 42.6326 - accuracy: 0.8576\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 1s 3ms/step - loss: 33.1221 - accuracy: 0.8626\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): # tf uses GPU by default, use this if your config is out of whack like mine is -C.J.\n",
    "                            # otherwise use - with tf.device('/GPU:0) or remove code from under with statement\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = [450, 450]),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'), # Dense operation is output = activation(dot(input, kernel) + bias)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),                \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    #print(images.shape, labels.shape)\n",
    "\n",
    "    model.fit(grayImages, grayLabels, epochs = 20, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create testing data\n",
    "grayCancerTest = get_gray_images(training_dir_base+fold_1_all_path)\n",
    "grayHealthyTest = get_gray_images(training_dir_base+fold_1_hem_path)\n",
    "\n",
    "testCancerLabels = [1 for _ in range(len(grayCancerTest))]\n",
    "testHealthLabels = [0 for _ in range(len(grayHealthyTest))]\n",
    "\n",
    "images, labels = grayCancerTest + grayHealthyTest, testCancerLabels + testHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "grayImagesTest = np.array(images).reshape(len(images), 450, 450)\n",
    "grayLabelsTest = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 10ms/step - loss: 173.9515 - accuracy: 0.7053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[173.95147705078125, 0.7052947282791138]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test model on test data\n",
    "model.evaluate(grayImagesTest, grayLabelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron: HSV Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsvCancer = get_hsv_images(training_dir_base+fold_0_all_path)\n",
    "hsvHealthy = get_hsv_images(training_dir_base+fold_0_hem_path)\n",
    "\n",
    "trainCancerLabels = [1 for _ in range(len(hsvCancer))]\n",
    "trainHealthLabels = [0 for _ in range(len(hsvHealthy))]\n",
    "\n",
    "images, labels = hsvCancer + hsvHealthy, trainCancerLabels + trainHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "hsvImages = np.array(images).reshape(len(images), 450, 450, 3)\n",
    "hsvLabels = np.array(labels)\n",
    "#plt.imshow(hsvImages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle labels and images in unison so they maintain their image-label correspondance\n",
    "randomize = np.arange(len(hsvImages))\n",
    "np.random.shuffle(randomize)\n",
    "hsvImages = hsvImages[randomize]\n",
    "hsvLabels = hsvLabels[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 18s 11ms/step - loss: 152.6490 - accuracy: 0.7033\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 166.2158 - accuracy: 0.7208\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 212.0763 - accuracy: 0.7213\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 123.9099 - accuracy: 0.7562\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 121.9310 - accuracy: 0.7542\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 128.8242 - accuracy: 0.7612\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 96.5776 - accuracy: 0.7922\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 88.0598 - accuracy: 0.8022\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 93.3836 - accuracy: 0.7897\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 101.8579 - accuracy: 0.7967\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = [450, 450, 3]),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'), # Dense operation is output = activation(dot(input, kernel) + bias)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),                \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(hsvImages, hsvLabels, epochs = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create testing data\n",
    "hsvCancerTest = get_hsv_images(training_dir_base+fold_1_all_path)\n",
    "hsvHealthyTest = get_hsv_images(training_dir_base+fold_1_hem_path)\n",
    "\n",
    "testCancerLabels = [1 for _ in range(len(hsvCancerTest))]\n",
    "testHealthLabels = [0 for _ in range(len(hsvHealthyTest))]\n",
    "\n",
    "images, labels = hsvCancerTest + hsvHealthyTest, testCancerLabels + testHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "hsvImagesTest = np.array(images).reshape(len(images), 450, 450, 3)\n",
    "hsvLabelsTest = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 36s 32ms/step - loss: 187.0238 - accuracy: 0.5874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[187.02383422851562, 0.5874125957489014]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test model on test data\n",
    "model.evaluate(hsvImagesTest, hsvLabelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron: Saturated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "satCancer = get_saturated_images(training_dir_base+fold_0_all_path)\n",
    "satHealthy = get_saturated_images(training_dir_base+fold_0_hem_path)\n",
    "\n",
    "trainCancerLabels = [1 for _ in range(len(satCancer))]\n",
    "trainHealthLabels = [0 for _ in range(len(satHealthy))]\n",
    "\n",
    "images, labels = satCancer + satHealthy, trainCancerLabels + trainHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "satImages = np.array(images).reshape(len(images), 450, 450, 3)\n",
    "satLabels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle labels and images in unison so they maintain their image-label correspondance\n",
    "randomize = np.arange(len(satImages))\n",
    "np.random.shuffle(randomize)\n",
    "satImages = satImages[randomize]\n",
    "satLabels = satLabels[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 9s 11ms/step - loss: 476.9868 - accuracy: 0.6968\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 282.3785 - accuracy: 0.7308\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 234.1632 - accuracy: 0.7517\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 302.7915 - accuracy: 0.7587\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 214.7651 - accuracy: 0.7782\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 325.8617 - accuracy: 0.7732\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 190.3839 - accuracy: 0.8117\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 118.4699 - accuracy: 0.8332\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 185.2514 - accuracy: 0.8262\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 2s 11ms/step - loss: 120.7472 - accuracy: 0.8467\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = [450, 450, 3]),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'), # Dense operation is output = activation(dot(input, kernel) + bias)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),                \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(satImages, satLabels, epochs = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron: Saturated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AdjustSaturation_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[450,450,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AdjustSaturation]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m### Create testing data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m satCancerTest \u001b[39m=\u001b[39m get_saturated_images(training_dir_base\u001b[39m+\u001b[39mfold_1_all_path)\n\u001b[1;32m----> 3\u001b[0m satHealthyTest \u001b[39m=\u001b[39m get_saturated_images(training_dir_base\u001b[39m+\u001b[39;49mfold_1_hem_path)\n\u001b[0;32m      5\u001b[0m testCancerLabels \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(satCancerTest))]\n\u001b[0;32m      6\u001b[0m testHealthLabels \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(satHealthyTest))]\n",
      "File \u001b[1;32ms:\\DataMining\\Data-Mining-Final-\\notebooks\\utils\\preprocessing.py:87\u001b[0m, in \u001b[0;36mget_saturated_images\u001b[1;34m(directory, size, imgTotal)\u001b[0m\n\u001b[0;32m     85\u001b[0m saturated_imgs \u001b[39m=\u001b[39m []\n\u001b[0;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m rgb_img \u001b[39min\u001b[39;00m image_array:\n\u001b[1;32m---> 87\u001b[0m     tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49madjust_saturation(rgb_img, \u001b[39m1\u001b[39;49m) \u001b[39m# use contrast factor of 0.5\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m (size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     89\u001b[0m         tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(tensor, [size, size])\n",
      "File \u001b[1;32mc:\\Users\\colto\\anaconda3\\envs\\testDM2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\colto\\anaconda3\\envs\\testDM2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AdjustSaturation_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[450,450,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AdjustSaturation]"
     ]
    }
   ],
   "source": [
    "### Create testing data\n",
    "satCancerTest = get_saturated_images(training_dir_base+fold_1_all_path)\n",
    "satHealthyTest = get_saturated_images(training_dir_base+fold_1_hem_path)\n",
    "\n",
    "testCancerLabels = [1 for _ in range(len(satCancerTest))]\n",
    "testHealthLabels = [0 for _ in range(len(satHealthyTest))]\n",
    "\n",
    "images, labels = satCancerTest + satHealthyTest, testCancerLabels + testHealthLabels # concatenate cancer and healthy images, as well as their labels into combined image and label vectors\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "\n",
    "# convert to np arrays for training\n",
    "satImagesTest = np.array(images).reshape(len(images), 450, 450, 3)\n",
    "satLabelsTest = np.array(labels)\n",
    "### usually works, currently expirencing resource exhausted error- plz try on your machine if you pull this -C.J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m### Test model on test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(hsvImagesTest, hsvLabelsTest)\n",
      "File \u001b[1;32mc:\\Users\\colto\\anaconda3\\envs\\testDM2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\colto\\anaconda3\\envs\\testDM2\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "### Test model on test data\n",
    "model.evaluate(hsvImagesTest, hsvLabelsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('testDM2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3bbd7400a4e540df9ec213ff5f9d80417bf0d5b3957fbd1d0937394fa3586a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
